{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b8d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9bcbdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\DELL\\\\Desktop\\\\endtoend-mlops\\\\projects\\\\covid_chest_classifier\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd18c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9916a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\DELL\\\\Desktop\\\\endtoend-mlops\\\\projects\\\\covid_chest_classifier'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a29203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_learning_rate: float\n",
    "    params_optimizer: str\n",
    "    params_loss_fn: str\n",
    "    params_device: str\n",
    "    params_patience: int   #NEW\n",
    "    num_classes: int\n",
    "    checkpoint_interval: int  \n",
    "    params_val_split: float = 0.2       # % of training data for validation     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7aa784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ccclassifier.constants import *\n",
    "from src.ccclassifier.utils.common import read_yaml, create_directories\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17bf7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        # create root artifacts directory\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"Covid19-dataset\")\n",
    "\n",
    "        create_directories([Path(training.root_dir)])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "\n",
    "            # all pulled from config.yaml:training\n",
    "            params_epochs=training.epochs,\n",
    "            params_batch_size=training.batch_size,\n",
    "            params_is_augmentation=getattr(training, \"augmentation\", False),\n",
    "            params_image_size=self.config.base_model.image_size,\n",
    "            params_learning_rate=training.learning_rate,\n",
    "            params_optimizer=training.optimizer,\n",
    "            params_loss_fn=training.loss_fn,\n",
    "            params_device=training.device,\n",
    "            params_val_split=getattr(training, \"val_split\", 0.2),   # <--- NEW\n",
    "            params_patience=getattr(training, \"patience\", 5),\n",
    "            checkpoint_interval=getattr(training, \"checkpoint_interval\", 5),\n",
    "            # num_classes from base_model\n",
    "            num_classes=self.config.base_model.num_classes\n",
    "            \n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1ce1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from pathlib import Path\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4bf5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.params_device)\n",
    "\n",
    "        # --- Build base ResNet18 ---\n",
    "        self.model = models.resnet18(weights=None)   # fresh model\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, self.config.num_classes)\n",
    "\n",
    "        # --- Load saved weights from stage 02 ---\n",
    "        state_dict = torch.load(\n",
    "            self.config.updated_base_model_path,\n",
    "            map_location=self.device,\n",
    "            weights_only=True   # fix warning\n",
    "        )\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # --- Loss function ---\n",
    "        if self.config.params_loss_fn == \"CrossEntropyLoss\":\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {self.config.params_loss_fn}\")\n",
    "\n",
    "        # --- Optimizer ---\n",
    "        if self.config.params_optimizer == \"Adam\":\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=self.config.params_learning_rate\n",
    "            )\n",
    "        elif self.config.params_optimizer == \"SGD\":\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(),\n",
    "                lr=self.config.params_learning_rate,\n",
    "                momentum=0.9\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported optimizer: {self.config.params_optimizer}\")\n",
    "\n",
    "        # --- Data transforms ---\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.Resize((self.config.params_image_size[0], self.config.params_image_size[1])),\n",
    "            transforms.RandomHorizontalFlip() if self.config.params_is_augmentation else transforms.Lambda(lambda x: x),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        val_transforms = transforms.Compose([\n",
    "            transforms.Resize((self.config.params_image_size[0], self.config.params_image_size[1])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # --- Datasets & loaders ---\n",
    "        full_train_dataset = datasets.ImageFolder(\n",
    "            root=str(self.config.training_data / \"train\"),\n",
    "            transform=train_transforms\n",
    "        )\n",
    "\n",
    "        # Split into train/val\n",
    "        val_size = int(0.2 * len(full_train_dataset))\n",
    "        train_size = len(full_train_dataset) - val_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "        # Final test set (untouched)\n",
    "        test_dataset = datasets.ImageFolder(\n",
    "            root=str(self.config.training_data / \"test\"),\n",
    "            transform=val_transforms\n",
    "        )\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    # Helper methods\n",
    "    def get_base_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def train_valid_generator(self):\n",
    "        return self.train_loader, self.val_loader\n",
    "    \n",
    "    start_epoch = 0\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    def train(self):\n",
    "        best_val_loss = float(\"inf\")\n",
    "        early_stop_counter = 0\n",
    "        start_epoch = 0   # >> always define a default\n",
    "\n",
    "        # --- Resume from latest checkpoint if available ---\n",
    "        import glob\n",
    "        checkpoints = glob.glob(str(self.config.root_dir / \"checkpoint_epoch_*.pth\"))\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=os.path.getctime)  # pick latest\n",
    "            checkpoint = torch.load(latest_checkpoint, map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_val_loss = checkpoint.get('best_val_loss', float(\"inf\"))  # resume best loss\n",
    "            early_stop_counter = checkpoint.get('early_stop_counter', 0)   # resume patience\n",
    "            print(f\">> Resumed training from {latest_checkpoint} (epoch {start_epoch})\")\n",
    "\n",
    "        # --- Training loop ---\n",
    "        for epoch in range(start_epoch, self.config.params_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.params_epochs}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "            # --- Training phase ---\n",
    "            self.model.train()\n",
    "            train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "            for images, labels in self.train_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * images.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_loss /= total\n",
    "            train_acc = 100. * correct / total\n",
    "\n",
    "            # --- Validation phase ---\n",
    "            self.model.eval()\n",
    "            val_loss, correct, total = 0, 0, 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in self.val_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_loss /= total\n",
    "            val_acc = 100. * correct / total\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "            # --- Save best model ---\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), self.config.trained_model_path)\n",
    "                print(f\">>> Saved best model weights to {self.config.trained_model_path}\")\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                print(f\"No improvement. Early stop counter: {early_stop_counter}/{self.config.params_patience}\")\n",
    "\n",
    "                if early_stop_counter >= self.config.params_patience:\n",
    "                    print(\">> Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "            # --- Save checkpoint every N epochs ---\n",
    "            if (epoch + 1) % self.config.checkpoint_interval == 0:\n",
    "                checkpoint_path = self.config.root_dir / f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch+1,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'loss': val_loss,\n",
    "                    'best_val_loss': best_val_loss,        #  save best loss\n",
    "                    'early_stop_counter': early_stop_counter,  # save patience counter\n",
    "                }, checkpoint_path)\n",
    "                print(f\">> Checkpoint saved at {checkpoint_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bbb037e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-07 09:04:52,277: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-07 09:04:52,289: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-07 09:04:52,290: INFO: common: created directory at: artifacts]\n",
      "[2025-09-07 09:04:52,292: INFO: common: created directory at: artifacts\\training]\n",
      "\n",
      "Epoch 1/23\n",
      "------------------------------\n",
      "Train Loss: 0.3976, Train Acc: 79.10%\n",
      "Val Loss: 11.5332, Val Acc: 52.00%\n",
      ">>> Saved best model weights to artifacts\\training\\model.pth\n",
      "\n",
      "Epoch 2/23\n",
      "------------------------------\n",
      "Train Loss: 0.2210, Train Acc: 94.53%\n",
      "Val Loss: 62.9482, Val Acc: 26.00%\n",
      "No improvement. Early stop counter: 1/5\n",
      "\n",
      "Epoch 3/23\n",
      "------------------------------\n",
      "Train Loss: 0.2335, Train Acc: 93.03%\n",
      "Val Loss: 36.9810, Val Acc: 26.00%\n",
      "No improvement. Early stop counter: 2/5\n",
      "\n",
      "Epoch 4/23\n",
      "------------------------------\n",
      "Train Loss: 0.0934, Train Acc: 96.52%\n",
      "Val Loss: 10.0689, Val Acc: 60.00%\n",
      ">>> Saved best model weights to artifacts\\training\\model.pth\n",
      "\n",
      "Epoch 5/23\n",
      "------------------------------\n",
      "Train Loss: 0.0644, Train Acc: 97.51%\n",
      "Val Loss: 1.8474, Val Acc: 82.00%\n",
      ">>> Saved best model weights to artifacts\\training\\model.pth\n",
      "💾 Checkpoint saved at artifacts\\training\\checkpoint_epoch_5.pth\n",
      "\n",
      "Epoch 6/23\n",
      "------------------------------\n",
      "Train Loss: 0.0912, Train Acc: 96.52%\n",
      "Val Loss: 0.0634, Val Acc: 96.00%\n",
      ">>> Saved best model weights to artifacts\\training\\model.pth\n",
      "\n",
      "Epoch 7/23\n",
      "------------------------------\n",
      "Train Loss: 0.0350, Train Acc: 98.51%\n",
      "Val Loss: 0.0582, Val Acc: 96.00%\n",
      ">>> Saved best model weights to artifacts\\training\\model.pth\n",
      "\n",
      "Epoch 8/23\n",
      "------------------------------\n",
      "Train Loss: 0.0862, Train Acc: 97.01%\n",
      "Val Loss: 0.5267, Val Acc: 82.00%\n",
      "No improvement. Early stop counter: 1/5\n",
      "\n",
      "Epoch 9/23\n",
      "------------------------------\n",
      "Train Loss: 0.1842, Train Acc: 93.03%\n",
      "Val Loss: 0.9488, Val Acc: 74.00%\n",
      "No improvement. Early stop counter: 2/5\n",
      "\n",
      "Epoch 10/23\n",
      "------------------------------\n",
      "Train Loss: 0.1842, Train Acc: 94.03%\n",
      "Val Loss: 6.5684, Val Acc: 48.00%\n",
      "No improvement. Early stop counter: 3/5\n",
      "💾 Checkpoint saved at artifacts\\training\\checkpoint_epoch_10.pth\n",
      "\n",
      "Epoch 11/23\n",
      "------------------------------\n",
      "Train Loss: 0.1561, Train Acc: 94.03%\n",
      "Val Loss: 0.5767, Val Acc: 82.00%\n",
      "No improvement. Early stop counter: 4/5\n",
      "\n",
      "Epoch 12/23\n",
      "------------------------------\n",
      "Train Loss: 0.0733, Train Acc: 97.51%\n",
      "Val Loss: 0.1257, Val Acc: 96.00%\n",
      "No improvement. Early stop counter: 5/5\n",
      "⏹️ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.get_base_model()\n",
    "    training.train_valid_generator()\n",
    "    training.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224de29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tubato_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
